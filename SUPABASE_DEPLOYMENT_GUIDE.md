# üöÄ Supabase Daily Scraper - Complete Deployment Guide

## Overview
This guide will help you deploy your web scraper to run daily at **6 PM AEST** and automatically upload data to Supabase.

---

## ‚úÖ What's Already Configured

‚úÖ **Scraper Code** - Fully functional with BeautifulSoup + requests  
‚úÖ **Supabase Integration** - Automatically uploads CSV data after scraping  
‚úÖ **Environment Secrets** - Supabase credentials stored securely  
‚úÖ **Deployment Type** - Configured for scheduled deployment  
‚úÖ **Data Storage** - CSV files saved to `scraped_data/` folder  

---

## üìã Step-by-Step Deployment

### Step 1: Create Supabase Table üóÑÔ∏è

**IMPORTANT: Do this BEFORE deploying!**

1. Go to your Supabase dashboard
2. Navigate to **SQL Editor**
3. Copy the contents of `SUPABASE_TABLE_SETUP.sql` file
4. Paste into the SQL Editor
5. Click **Run** to create the table
6. Verify the table was created in the **Table Editor**

**Quick SQL (Copy & Paste):**
```sql
CREATE TABLE IF NOT EXISTS public.globe_daily_data (
    id BIGSERIAL PRIMARY KEY,
    url TEXT,
    product_name TEXT,
    product_code TEXT,
    sku TEXT,
    price TEXT,
    availability TEXT,
    product_quantity TEXT,
    description TEXT,
    status TEXT,
    scraped_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX IF NOT EXISTS idx_globe_daily_data_scraped_at ON public.globe_daily_data(scraped_at DESC);
```

### Step 2: Deploy to Replit üöÄ

1. Click the **Deploy** button in the top right corner
2. You'll see the deployment configuration:
   - Type: **Scheduled**
   - Command: `python main.py`
3. Click **Deploy** to create your deployment

### Step 3: Set Schedule for 6 PM AEST ‚è∞

**Important:** AEST is UTC+10 (or UTC+11 during daylight saving)

For **6 PM AEST (18:00)**, you need to convert to UTC:
- **AEST (UTC+10)**: 6 PM AEST = 8 AM UTC
- **AEDT (UTC+11, daylight saving)**: 6 PM AEDT = 7 AM UTC

**Cron Expression for 6 PM AEST (Standard Time):**
```
0 8 * * *
```

**Cron Expression for 6 PM AEDT (Daylight Saving):**
```
0 7 * * *
```

**Where to Set This:**
1. Go to your deployment settings
2. Find the **Schedule** or **Cron** configuration
3. Enter the cron expression above
4. Save the schedule

---

## üîÑ How It Works

### Daily Workflow:
1. **6:00 PM AEST** - Scraper starts automatically
2. **Scraping** - Extracts data from all 119 product URLs
3. **Save CSV** - Creates timestamped file: `scraped_products_YYYYMMDD_HHMMSS.csv`
4. **Upload to Supabase** - Automatically inserts data into your database table
5. **Complete** - Check logs for summary

### Data Flow:
```
CSV URLs ‚Üí Web Scraper ‚Üí Local CSV File ‚Üí Supabase Table
                ‚Üì
        Product Information:
        - Product Name
        - Price
        - SKU/Product Code
        - Availability
        - Description
        - Scraped Timestamp
```

---

## üóÑÔ∏è Supabase Configuration

### Environment Variables (Already Set):
- ‚úÖ `SUPABASE_URL` - Your Supabase project URL
- ‚úÖ `SUPABASE_KEY` - Service role key for database access
- ‚úÖ `SUPABASE_SCHEMA` - Database schema (public)
- ‚úÖ `SUPABASE_TABLE` - Target table (globe_daily_data)

### Database Table Structure

Your Supabase table `globe_daily_data` should have these columns:

```sql
CREATE TABLE globe_daily_data (
    id BIGSERIAL PRIMARY KEY,
    url TEXT,
    product_name TEXT,
    product_code TEXT,
    sku TEXT,
    price TEXT,
    availability TEXT,
    product_quantity TEXT,
    description TEXT,
    status TEXT,
    scraped_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**To create this table:**
1. Go to your Supabase dashboard
2. Navigate to **SQL Editor**
3. Run the SQL above
4. Click **Run** to create the table

---

## üìä Output Files

### Local CSV Files:
```
scraped_data/
‚îú‚îÄ‚îÄ scraped_products_20251027_180000.csv  (Today 6 PM)
‚îú‚îÄ‚îÄ scraped_products_20251028_180000.csv  (Tomorrow 6 PM)
‚îî‚îÄ‚îÄ scraped_products_20251029_180000.csv  (Next day 6 PM)
```

### Supabase Database:
- All CSV data is uploaded to `public.globe_daily_data` table
- Each record includes a `scraped_at` timestamp
- Query historical data easily with SQL

---

## üîç Monitoring Your Deployment

### Check Deployment Status:
1. Go to **Deployments** tab in Replit
2. Click on your active deployment
3. View **Status** (should show "Active")

### View Logs:
1. In deployment dashboard, click **Logs**
2. Look for:
   - `‚úì Successfully scraped X products`
   - `‚úÖ Successfully uploaded X records to Supabase`
3. Check for any error messages

### Query Supabase Data:
```sql
-- View latest scrape
SELECT * FROM globe_daily_data 
ORDER BY scraped_at DESC 
LIMIT 100;

-- Count products per day
SELECT DATE(scraped_at), COUNT(*) 
FROM globe_daily_data 
GROUP BY DATE(scraped_at) 
ORDER BY DATE(scraped_at) DESC;

-- Find price changes
SELECT 
    product_name, 
    price, 
    scraped_at 
FROM globe_daily_data 
WHERE product_name = 'YOUR_PRODUCT_NAME'
ORDER BY scraped_at DESC;
```

---

## ‚öôÔ∏è Schedule Reference

| Time (AEST) | UTC Time | Cron Expression | Notes |
|-------------|----------|----------------|-------|
| 6:00 PM | 8:00 AM | `0 8 * * *` | Standard time (UTC+10) |
| 6:00 PM | 7:00 AM | `0 7 * * *` | Daylight saving (UTC+11) |
| 12:00 PM | 2:00 AM | `0 2 * * *` | Noon AEST |
| 9:00 AM | 11:00 PM* | `0 23 * * *` | *Previous day UTC |
| Twice daily<br>(9AM & 6PM) | 11PM & 8AM | `0 23,8 * * *` | Both times |

---

## üõ†Ô∏è Troubleshooting

### Scraper Not Running?
**Check:**
- ‚úì Deployment status is "Active"
- ‚úì Schedule is correctly set in deployment
- ‚úì No error messages in logs

**Fix:**
- Verify cron expression is correct
- Check deployment logs for errors
- Ensure schedule is saved

### No Data in Supabase?
**Check:**
- ‚úì Table `globe_daily_data` exists
- ‚úì Environment variables are set correctly
- ‚úì Service role key has insert permissions

**Fix:**
- Create table using SQL above
- Verify secrets in Replit settings
- Check Supabase logs for permission errors

### Cookies Expired?
**Symptoms:**
- Many 401/403 errors in logs
- Products not scraping successfully

**Fix:**
1. Export fresh cookies from browser
2. Update `cookies_json` in `main.py`
3. Redeploy the application

### Wrong Timezone?
**Fix:**
- AEST is UTC+10 (add 10 hours to AEST for UTC)
- During daylight saving (Oct-Apr): Use UTC+11
- Use online cron calculator for verification

---

## üìà Performance Expectations

- **Total Products**: 119
- **Scraping Time**: ~2 minutes
- **Upload Time**: <10 seconds
- **Success Rate**: ~98% (some URLs may be unavailable)
- **Data Size**: ~30-40 KB per CSV file

---

## üîê Security Best Practices

‚úÖ **Environment Secrets** - Never hardcode credentials  
‚úÖ **Service Role Key** - Stored securely in Replit  
‚úÖ **HTTPS Only** - All connections encrypted  
‚úÖ **No Credentials in Code** - Using env variables  
‚úÖ **Read from Secrets** - At runtime only  

---

## üéØ Next Steps

### After Deployment:
1. ‚úÖ Wait for first scheduled run (6 PM AEST)
2. ‚úÖ Check deployment logs for success
3. ‚úÖ Verify data appears in Supabase table
4. ‚úÖ Set up Supabase alerts (optional)
5. ‚úÖ Create data visualizations (optional)

### Optional Enhancements:
- **Email Notifications** - Get notified when scrape completes
- **Error Alerts** - Send alerts if scraping fails
- **Data Analysis** - Track price changes over time
- **Dashboard** - Build a dashboard to visualize trends
- **Deduplication** - Remove duplicate entries in database

---

## üìû Support

### If You Need Help:
1. **Check Logs First** - Most issues show in deployment logs
2. **Verify Schedule** - Ensure cron expression is correct
3. **Test Manually** - Run `python main.py` in shell to debug
4. **Check Supabase** - Verify table exists and has correct permissions

### Common Questions:

**Q: Can I change the schedule after deployment?**  
A: Yes! Just update the cron expression in deployment settings.

**Q: How do I see historical data?**  
A: Query the Supabase table using SQL editor or build a dashboard.

**Q: What if some products fail?**  
A: The scraper continues and logs errors. Check logs for which URLs failed.

**Q: Can I run the scraper manually?**  
A: Yes! Use the Replit shell and run `python main.py`.

---

## ‚ú® You're All Set!

Your scraper is configured to:
- ‚úÖ Run daily at 6 PM AEST
- ‚úÖ Scrape all 119 products
- ‚úÖ Save CSV files locally
- ‚úÖ Upload data to Supabase automatically

**Just click Deploy and set your schedule!** üöÄ

---

*Last Updated: October 27, 2025*
